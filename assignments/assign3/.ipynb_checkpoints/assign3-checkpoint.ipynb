{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from python_speech_features import mfcc\n",
    "from python_speech_features import delta\n",
    "from python_speech_features import logfbank\n",
    "import scipy.io.wavfile as wav\n",
    "import sys\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10\n",
      "10.0% complete\n",
      "20.0% complete\n",
      "30.0% complete\n",
      "40.0% complete\n",
      "50.0% complete\n",
      "60.0% complete\n",
      "70.0% complete\n",
      "80.0% complete\n",
      "90.0% complete\n",
      "100.0% complete\n",
      "The one hot signature for this word is: [1 0 0 0 0 0 0 0 0 0]\n",
      "The number of samples for this word is: 1000\n",
      "2/10\n",
      "10.0% complete\n",
      "20.0% complete\n",
      "30.0% complete\n",
      "40.0% complete\n",
      "50.0% complete\n",
      "60.0% complete\n",
      "70.0% complete\n",
      "70.0% complete\n",
      "80.0% complete\n",
      "80.0% complete\n",
      "80.0% complete\n",
      "80.0% complete\n",
      "90.0% complete\n",
      "90.0% complete\n",
      "100.0% complete\n",
      "The one hot signature for this word is: [0 1 0 0 0 0 0 0 0 0]\n",
      "The number of samples for this word is: 1000\n",
      "3/10\n",
      "0.0% complete\n",
      "10.0% complete\n",
      "20.0% complete\n",
      "30.0% complete\n",
      "30.0% complete\n",
      "40.0% complete\n",
      "50.0% complete\n",
      "60.0% complete\n",
      "70.0% complete\n",
      "80.0% complete\n",
      "90.0% complete\n",
      "100.0% complete\n",
      "The one hot signature for this word is: [0 0 1 0 0 0 0 0 0 0]\n",
      "The number of samples for this word is: 1000\n",
      "4/10\n",
      "10.0% complete\n",
      "10.0% complete\n",
      "20.0% complete\n",
      "30.0% complete\n",
      "30.0% complete\n",
      "40.0% complete\n",
      "50.0% complete\n",
      "60.0% complete\n",
      "70.0% complete\n",
      "80.0% complete\n",
      "90.0% complete\n",
      "100.0% complete\n",
      "The one hot signature for this word is: [0 0 0 1 0 0 0 0 0 0]\n",
      "The number of samples for this word is: 1000\n",
      "5/10\n",
      "10.0% complete\n",
      "20.0% complete\n",
      "30.0% complete\n",
      "40.0% complete\n",
      "50.0% complete\n",
      "60.0% complete\n",
      "60.0% complete\n",
      "70.0% complete\n",
      "80.0% complete\n",
      "90.0% complete\n",
      "100.0% complete\n",
      "The one hot signature for this word is: [0 0 0 0 1 0 0 0 0 0]\n",
      "The number of samples for this word is: 1000\n",
      "6/10\n",
      "0.0% complete\n",
      "10.0% complete\n",
      "20.0% complete\n",
      "30.0% complete\n",
      "40.0% complete\n",
      "50.0% complete\n",
      "60.0% complete\n",
      "70.0% complete\n",
      "80.0% complete\n",
      "90.0% complete\n",
      "100.0% complete\n",
      "The one hot signature for this word is: [0 0 0 0 0 1 0 0 0 0]\n",
      "The number of samples for this word is: 1000\n",
      "7/10\n",
      "10.0% complete\n",
      "20.0% complete\n",
      "30.0% complete\n",
      "40.0% complete\n",
      "50.0% complete\n",
      "60.0% complete\n",
      "70.0% complete\n",
      "80.0% complete\n",
      "90.0% complete\n",
      "100.0% complete\n",
      "The one hot signature for this word is: [0 0 0 0 0 0 1 0 0 0]\n",
      "The number of samples for this word is: 1000\n",
      "8/10\n",
      "10.0% complete\n",
      "20.0% complete\n",
      "30.0% complete\n",
      "40.0% complete\n",
      "50.0% complete\n",
      "60.0% complete\n",
      "70.0% complete\n",
      "80.0% complete\n",
      "90.0% complete\n",
      "100.0% complete\n",
      "The one hot signature for this word is: [0 0 0 0 0 0 0 1 0 0]\n",
      "The number of samples for this word is: 1000\n",
      "9/10\n",
      "10.0% complete\n",
      "20.0% complete\n",
      "20.0% complete\n",
      "30.0% complete\n",
      "40.0% complete\n",
      "50.0% complete\n",
      "60.0% complete\n",
      "60.0% complete\n",
      "60.0% complete\n",
      "70.0% complete\n",
      "80.0% complete\n",
      "90.0% complete\n",
      "100.0% complete\n",
      "The one hot signature for this word is: [0 0 0 0 0 0 0 0 1 0]\n",
      "The number of samples for this word is: 1000\n",
      "10/10\n",
      "10.0% complete\n",
      "20.0% complete\n",
      "30.0% complete\n",
      "40.0% complete\n",
      "50.0% complete\n",
      "60.0% complete\n",
      "70.0% complete\n",
      "80.0% complete\n",
      "90.0% complete\n",
      "100.0% complete\n",
      "The one hot signature for this word is: [0 0 0 0 0 0 0 0 0 1]\n",
      "The number of samples for this word is: 1000\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "NUM_LABELS = 10\n",
    "\n",
    "def genOneHot(indexToBeOne):\n",
    "    global NUM_LABELS\n",
    "    oneHot = []\n",
    "    for i in range(NUM_LABELS):\n",
    "        if i == indexToBeOne:\n",
    "            oneHot.append(1)\n",
    "        else:\n",
    "            oneHot.append(0)\n",
    "    return np.array(oneHot)\n",
    "\n",
    "def extractFeatures(raw, sr):\n",
    "    stft = np.abs(librosa.stft(raw))\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=raw, sr=sr, n_mfcc=40).T,axis=0)\n",
    "    mel = np.mean(librosa.feature.melspectrogram(raw, sr=sr).T,axis=0)\n",
    "#     chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sr).T,axis=0)\n",
    "#     contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sr).T,axis=0)\n",
    "#     tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(raw),sr=sr).T,axis=0)\n",
    "    \n",
    "    return mel\n",
    "    \n",
    "def loadSoundFiles(filePath, index):\n",
    "    IDEAL_SIZE = 22050\n",
    "    NUM_SAMPLES = 1000\n",
    "    oneHots = []\n",
    "    soundFiles = os.listdir(filePath)\n",
    "    features = np.empty((0, 128))\n",
    "    \n",
    "    counter = 0\n",
    "    for soundFile in soundFiles:\n",
    "        raw, sr = librosa.load(filePath + soundFile)\n",
    "        if len(raw) == IDEAL_SIZE:\n",
    "            mel = extractFeatures(raw, sr)\n",
    "            internalFeatures = np.hstack([mel])\n",
    "            features = np.vstack([features, internalFeatures])\n",
    "            counter += 1\n",
    "            \n",
    "            oneHot = genOneHot(index)\n",
    "            if len(oneHots) == 0:\n",
    "                oneHots = oneHot\n",
    "            else:\n",
    "                oneHots = np.vstack((oneHots, oneHot))\n",
    "        \n",
    "            if (counter % (NUM_SAMPLES / 10)) == 0:\n",
    "                print(str((counter/NUM_SAMPLES) * 100) + \"% complete\")\n",
    "        \n",
    "            if counter == NUM_SAMPLES:\n",
    "                break\n",
    "            \n",
    "    print(\"The one hot signature for this word is: \" + str(oneHots[0]))\n",
    "    print(\"The number of samples for this word is: \" + str(len(features)))\n",
    "        \n",
    "    return np.array(features), oneHots\n",
    "    \n",
    "data = []\n",
    "oneHots = []\n",
    "rootDir = \"./train/audio/\"\n",
    "acceptedWords = [\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\"]\n",
    "count = 1\n",
    "for acceptedWord in acceptedWords:\n",
    "    print(str(count) + \"/\" + str(NUM_LABELS))\n",
    "    features, oneHot = loadSoundFiles(rootDir + acceptedWord + \"/\", count-1)\n",
    "    data.append(features)\n",
    "    oneHots.append(oneHot)\n",
    "    count += 1\n",
    "    \n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished formatting the data to remove bias and to be ready for training\n"
     ]
    }
   ],
   "source": [
    "dataTraining = []\n",
    "trainingLabels = []\n",
    "dataTest = []\n",
    "testLabels = []\n",
    "\n",
    "minSamples = 1000000\n",
    "for training in data:\n",
    "    if minSamples > len(training):\n",
    "        minSamples = len(training)\n",
    "\n",
    "numTrainingSamples = int(minSamples * 0.8)\n",
    "numTestSamples = int(minSamples * 0.2)\n",
    "\n",
    "for datasubset in data:\n",
    "    if len(dataTraining) == 0:\n",
    "        dataTraining = datasubset[:numTrainingSamples]\n",
    "        dataTest = datasubset[numTrainingSamples:numTrainingSamples+numTestSamples]\n",
    "    else:\n",
    "        dataTraining = np.vstack((dataTraining, datasubset[:numTrainingSamples]))\n",
    "        dataTest = np.vstack((dataTest, datasubset[numTrainingSamples:numTrainingSamples+numTestSamples]))\n",
    "               \n",
    "for oneHotInfo in oneHots:\n",
    "    if len(trainingLabels) == 0:\n",
    "        trainingLabels = oneHotInfo[:numTrainingSamples]\n",
    "        testLabels = oneHotInfo[numTrainingSamples:numTrainingSamples+numTestSamples]\n",
    "    else:\n",
    "        trainingLabels = np.vstack((trainingLabels, oneHotInfo[:numTrainingSamples]))\n",
    "        testLabels = np.vstack((testLabels, oneHotInfo[numTrainingSamples:numTrainingSamples+numTestSamples]))\n",
    "\n",
    "print(\"Finished formatting the data to remove bias and to be ready for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized the order of the data and labels keeping the relationship 1 to 1\n"
     ]
    }
   ],
   "source": [
    "dataTraining, trainingLabels = shuffle(dataTraining, trainingLabels)\n",
    "dataTest, testLabels = shuffle(dataTest, testLabels)\n",
    "print(\"Randomized the order of the data and labels keeping the relationship 1 to 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1% complete: Training Error = 0.962552. Cross Validation Error = 3.22797\n",
      "2% complete: Training Error = 0.778844. Cross Validation Error = 3.00522\n",
      "3% complete: Training Error = 0.942308. Cross Validation Error = 3.79976\n",
      "4% complete: Training Error = 0.806947. Cross Validation Error = 3.25792\n",
      "5% complete: Training Error = 0.752114. Cross Validation Error = 3.06435\n",
      "6% complete: Training Error = 0.715763. Cross Validation Error = 2.80659\n",
      "7% complete: Training Error = 0.702454. Cross Validation Error = 3.7587\n",
      "8% complete: Training Error = 0.649847. Cross Validation Error = 3.98267\n",
      "9% complete: Training Error = 0.58479. Cross Validation Error = 4.60736\n",
      "10% complete: Training Error = 0.56113. Cross Validation Error = 3.35113\n",
      "11% complete: Training Error = 0.517044. Cross Validation Error = 3.7597\n",
      "12% complete: Training Error = 0.594371. Cross Validation Error = 3.07002\n",
      "13% complete: Training Error = 0.368475. Cross Validation Error = 3.54908\n",
      "14% complete: Training Error = 0.499825. Cross Validation Error = 2.67216\n",
      "15% complete: Training Error = 0.544341. Cross Validation Error = 3.85559\n",
      "16% complete: Training Error = 0.47031. Cross Validation Error = 3.27652\n",
      "17% complete: Training Error = 0.544039. Cross Validation Error = 4.86481\n",
      "18% complete: Training Error = 0.527376. Cross Validation Error = 4.16745\n",
      "19% complete: Training Error = 0.452667. Cross Validation Error = 4.44615\n",
      "20% complete: Training Error = 0.420138. Cross Validation Error = 3.48128\n",
      "21% complete: Training Error = 0.483401. Cross Validation Error = 4.66818\n",
      "22% complete: Training Error = 0.492267. Cross Validation Error = 3.10517\n",
      "23% complete: Training Error = 0.461802. Cross Validation Error = 5.25069\n",
      "24% complete: Training Error = 0.408882. Cross Validation Error = 3.54336\n",
      "25% complete: Training Error = 0.456012. Cross Validation Error = 9.99574\n",
      "26% complete: Training Error = 0.380854. Cross Validation Error = 2.97438\n",
      "27% complete: Training Error = 0.480406. Cross Validation Error = 5.38881\n",
      "28% complete: Training Error = 0.450959. Cross Validation Error = 3.89766\n",
      "29% complete: Training Error = 0.434086. Cross Validation Error = 3.97982\n",
      "30% complete: Training Error = 0.395967. Cross Validation Error = 2.51532\n",
      "31% complete: Training Error = 0.390124. Cross Validation Error = 4.42009\n",
      "32% complete: Training Error = 0.475914. Cross Validation Error = 3.65225\n",
      "33% complete: Training Error = 0.40752. Cross Validation Error = 2.29193\n",
      "34% complete: Training Error = 0.46098. Cross Validation Error = 2.84031\n",
      "35% complete: Training Error = 0.397038. Cross Validation Error = 4.33083\n",
      "36% complete: Training Error = 0.366467. Cross Validation Error = 3.76232\n",
      "37% complete: Training Error = 0.429615. Cross Validation Error = 2.19678\n",
      "38% complete: Training Error = 0.440087. Cross Validation Error = 3.09427\n",
      "39% complete: Training Error = 0.357295. Cross Validation Error = 2.09923\n",
      "40% complete: Training Error = 0.404674. Cross Validation Error = 3.45584\n",
      "41% complete: Training Error = 0.48972. Cross Validation Error = 4.45529\n",
      "42% complete: Training Error = 0.376671. Cross Validation Error = 3.02387\n",
      "43% complete: Training Error = 0.316965. Cross Validation Error = 2.85775\n",
      "44% complete: Training Error = 0.425027. Cross Validation Error = 3.63607\n",
      "45% complete: Training Error = 0.367587. Cross Validation Error = 5.39278\n",
      "46% complete: Training Error = 0.392049. Cross Validation Error = 3.54838\n",
      "47% complete: Training Error = 0.318963. Cross Validation Error = 2.80292\n",
      "48% complete: Training Error = 0.402436. Cross Validation Error = 2.88031\n",
      "49% complete: Training Error = 0.328025. Cross Validation Error = 3.75985\n",
      "50% complete: Training Error = 0.308473. Cross Validation Error = 4.84384\n",
      "51% complete: Training Error = 0.333545. Cross Validation Error = 2.63302\n",
      "52% complete: Training Error = 0.265225. Cross Validation Error = 4.70067\n",
      "53% complete: Training Error = 0.306601. Cross Validation Error = 2.52307\n",
      "54% complete: Training Error = 0.277376. Cross Validation Error = 4.39688\n",
      "55% complete: Training Error = 0.316053. Cross Validation Error = 2.45249\n",
      "56% complete: Training Error = 0.334761. Cross Validation Error = 4.87291\n",
      "57% complete: Training Error = 0.324236. Cross Validation Error = 3.86413\n",
      "58% complete: Training Error = 0.314982. Cross Validation Error = 7.79017\n",
      "59% complete: Training Error = 0.356768. Cross Validation Error = 3.55361\n",
      "60% complete: Training Error = 0.266998. Cross Validation Error = 5.72032\n",
      "61% complete: Training Error = 0.290376. Cross Validation Error = 2.09984\n",
      "62% complete: Training Error = 0.332068. Cross Validation Error = 4.48657\n",
      "63% complete: Training Error = 0.321517. Cross Validation Error = 4.39996\n",
      "64% complete: Training Error = 0.31856. Cross Validation Error = 3.40317\n",
      "65% complete: Training Error = 0.328454. Cross Validation Error = 2.53344\n",
      "66% complete: Training Error = 0.348173. Cross Validation Error = 3.12829\n",
      "67% complete: Training Error = 0.364342. Cross Validation Error = 3.40792\n",
      "68% complete: Training Error = 0.27347. Cross Validation Error = 2.25429\n",
      "69% complete: Training Error = 0.391563. Cross Validation Error = 3.37517\n",
      "70% complete: Training Error = 0.317621. Cross Validation Error = 3.63629\n",
      "71% complete: Training Error = 0.299489. Cross Validation Error = 3.89144\n",
      "72% complete: Training Error = 0.313086. Cross Validation Error = 3.50656\n",
      "73% complete: Training Error = 0.30564. Cross Validation Error = 3.8886\n",
      "74% complete: Training Error = 0.257026. Cross Validation Error = 2.57889\n",
      "75% complete: Training Error = 0.206461. Cross Validation Error = 3.14317\n",
      "76% complete: Training Error = 0.356611. Cross Validation Error = 2.18603\n",
      "77% complete: Training Error = 0.34956. Cross Validation Error = 2.79777\n",
      "78% complete: Training Error = 0.30768. Cross Validation Error = 2.48311\n",
      "79% complete: Training Error = 0.200366. Cross Validation Error = 3.69953\n",
      "80% complete: Training Error = 0.264091. Cross Validation Error = 4.54526\n",
      "81% complete: Training Error = 0.357208. Cross Validation Error = 3.39595\n",
      "82% complete: Training Error = 0.336516. Cross Validation Error = 3.51097\n",
      "83% complete: Training Error = 0.221464. Cross Validation Error = 2.14088\n",
      "84% complete: Training Error = 0.309524. Cross Validation Error = 5.43499\n",
      "85% complete: Training Error = 0.30388. Cross Validation Error = 3.32294\n",
      "86% complete: Training Error = 0.310804. Cross Validation Error = 2.23522\n",
      "87% complete: Training Error = 0.27845. Cross Validation Error = 2.71038\n",
      "88% complete: Training Error = 0.306849. Cross Validation Error = 2.70721\n",
      "89% complete: Training Error = 0.271993. Cross Validation Error = 3.48839\n",
      "90% complete: Training Error = 0.229217. Cross Validation Error = 2.03998\n",
      "91% complete: Training Error = 0.216171. Cross Validation Error = 2.81747\n",
      "92% complete: Training Error = 0.450258. Cross Validation Error = 7.81466\n",
      "93% complete: Training Error = 0.306015. Cross Validation Error = 3.05393\n",
      "94% complete: Training Error = 0.326581. Cross Validation Error = 2.94761\n",
      "95% complete: Training Error = 0.227898. Cross Validation Error = 2.53749\n",
      "96% complete: Training Error = 0.3312. Cross Validation Error = 3.57125\n",
      "97% complete: Training Error = 0.298995. Cross Validation Error = 3.70482\n",
      "98% complete: Training Error = 0.239836. Cross Validation Error = 2.81838\n",
      "99% complete: Training Error = 0.25105. Cross Validation Error = 2.18441\n",
      "100% complete: Training Error = 0.311728. Cross Validation Error = 3.47236\n"
     ]
    }
   ],
   "source": [
    "def getBatch(data, labels, batchSize):\n",
    "    randomIndexes = np.random.choice(len(data), batchSize)\n",
    "    return data[randomIndexes], labels[randomIndexes]\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "NUM_SAMPLES = 128\n",
    "NUM_CLASSIFICATIONS = NUM_LABELS\n",
    "FIRST_LAYER_OUTPUT = 100\n",
    "RATE = 0.01\n",
    "BATCH_SIZE = 200\n",
    "EPOCHS = 5000\n",
    "ITERATIONS_PER_EPOCH = 100\n",
    "\n",
    "# Shape = (N x 22050)\n",
    "x = tf.placeholder(tf.float32, (None, NUM_SAMPLES))\n",
    "# Shape = (N x 2)\n",
    "ref = tf.placeholder(tf.float32, (None, NUM_CLASSIFICATIONS))\n",
    "# Calculate the logits\n",
    "logits1 = tf.layers.dense(inputs=x, units=FIRST_LAYER_OUTPUT, activation=tf.nn.relu, name=\"L1\")\n",
    "logits2 = tf.layers.dense(inputs=logits1, units=NUM_CLASSIFICATIONS, activation=None, name=\"L2\")\n",
    "\n",
    "# The mean cross entropy as the cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits2, labels=ref))\n",
    "\n",
    "# Initialize the tensorflow session\n",
    "optimizer = tf.train.GradientDescentOptimizer(RATE).minimize(cost)\n",
    "s = tf.Session()\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "# Do the training\n",
    "count = 0\n",
    "for _ in range(EPOCHS):\n",
    "    inputData, correctAns = getBatch(dataTraining, trainingLabels, BATCH_SIZE)\n",
    "    crossData, crossAns = getBatch(dataTraining, trainingLabels, BATCH_SIZE)\n",
    "    for _ in range(ITERATIONS_PER_EPOCH):\n",
    "        err, _ = s.run((cost, optimizer), feed_dict={x: inputData, ref: correctAns})\n",
    "        count += 1\n",
    "        if (count % ((EPOCHS * ITERATIONS_PER_EPOCH) / 100)) == 0:\n",
    "            crossErr = s.run((cost), feed_dict={x: crossData, ref: crossAns})\n",
    "            print(str(int(((count / (EPOCHS * ITERATIONS_PER_EPOCH)) * 100) + 0.5)) + \"% complete: Training Error = \" + str(err) + \". Cross Validation Error = \" + str(crossErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The neural network is 69.65% accurate on the training data\n",
      "['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
      "[ 81.875  67.875  65.5    58.25   66.75   66.25   74.25   78.875  70.5\n",
      "  66.375]\n"
     ]
    }
   ],
   "source": [
    "# Take the softmax to convert the logit value to a percentage guess\n",
    "probability = tf.nn.softmax(logits2)\n",
    "# Take the highest probability value as the neural network's guess\n",
    "prediction = tf.argmax((probability), axis=1)\n",
    "# Check how accurate the training is\n",
    "guesses = s.run((prediction), feed_dict={x: dataTraining})\n",
    "\n",
    "count = 0\n",
    "corr = 0\n",
    "categoriesRight = np.zeros(10)\n",
    "for trainingLabel in trainingLabels:\n",
    "    if (np.argmax(trainingLabel)) == guesses[count]:\n",
    "        corr += 1\n",
    "        categoriesRight[np.argmax(trainingLabel)] += 1\n",
    "    count += 1\n",
    "\n",
    "print(\"The neural network is \" + str((corr / len(guesses)) * 100) + \"% accurate on the training data\")\n",
    "print(acceptedWords)\n",
    "count = 0\n",
    "print((categoriesRight / numTrainingSamples) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The neural network is 40.25% accurate on the test data\n",
      "['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
      "[ 60.   34.   41.5  34.   34.   32.   37.   51.5  37.   41.5]\n"
     ]
    }
   ],
   "source": [
    "# Take the softmax to convert the logit value to a percentage guess\n",
    "probability = tf.nn.softmax(logits2)\n",
    "# Take the highest probability value as the neural network's guess\n",
    "prediction = tf.argmax((probability), axis=1)\n",
    "# Check how accurate the test is\n",
    "guesses = s.run((prediction), feed_dict={x: dataTest})\n",
    "\n",
    "count = 0\n",
    "corr = 0\n",
    "categoriesRight = np.zeros(10)\n",
    "for testLabel in testLabels:\n",
    "    if (np.argmax(testLabel)) == guesses[count]:\n",
    "        corr += 1\n",
    "        categoriesRight[np.argmax(testLabel)] += 1\n",
    "    count += 1\n",
    "    \n",
    "print(\"The neural network is \" + str((corr / len(guesses)) * 100) + \"% accurate on the test data\")\n",
    "print(acceptedWords)\n",
    "print((categoriesRight / numTestSamples) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder_8' with dtype float\n\t [[Node: Placeholder_8 = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'Placeholder_8', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-236-c082f27d4590>\", line 37, in <module>\n    keep_prob = tf.placeholder(tf.float32)\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1548, in placeholder\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2094, in _placeholder\n    name=name)\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_8' with dtype float\n\t [[Node: Placeholder_8 = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_8' with dtype float\n\t [[Node: Placeholder_8 = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-236-c082f27d4590>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdataTraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrainingLabels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_8' with dtype float\n\t [[Node: Placeholder_8 = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'Placeholder_8', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-236-c082f27d4590>\", line 37, in <module>\n    keep_prob = tf.placeholder(tf.float32)\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1548, in placeholder\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2094, in _placeholder\n    name=name)\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_8' with dtype float\n\t [[Node: Placeholder_8 = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 200\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "subAudio = tf.reshape(x, [-1, 16, 8, 1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_fc1 = weight_variable([4 * 2 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 4 * 2 * 64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "convolution = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "ref = tf.placeholder(tf.float32, (None, NUM_CLASSIFICATIONS))\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=convolution, labels=ref))\n",
    "\n",
    "s = tf.Session()\n",
    "optimizer = tf.train.GradientDescentOptimizer(RATE).minimize(cost)\n",
    "s.run(tf.global_variables_initializer())\n",
    "err, _ = s.run((cost, optimizer), feed_dict={x:dataTraining, ref:trainingLabels})\n",
    "print(err)\n",
    "\n",
    "# Initialize the tensorflow session\n",
    "# optimizer = tf.train.GradientDescentOptimizer(RATE).minimize(cost)\n",
    "# s = tf.Session()\n",
    "# s.run(tf.global_variables_initializer())\n",
    "\n",
    "# Do the training\n",
    "# count = 0\n",
    "# for _ in range(EPOCHS):\n",
    "#     inputData, correctAns = getBatch(dataTraining, trainingLabels, BATCH_SIZE)\n",
    "#     crossData, crossAns = getBatch(dataTraining, trainingLabels, BATCH_SIZE)\n",
    "#     for _ in range(ITERATIONS_PER_EPOCH):\n",
    "#         err, _ = s.run((cost, optimizer), feed_dict={x: inputData, ref: correctAns})\n",
    "#         count += 1\n",
    "#         if (count % ((EPOCHS * ITERATIONS_PER_EPOCH) / 100)) == 0:\n",
    "#             crossErr = s.run((cost), feed_dict={x: crossData, ref: crossAns})\n",
    "#             print(str(int(((count / (EPOCHS * ITERATIONS_PER_EPOCH)) * 100) + 0.5)) + \"% complete: Training Error = \" + str(err) + \". Cross Validation Error = \" + str(crossErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
