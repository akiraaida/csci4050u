{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from python_speech_features import mfcc\n",
    "from python_speech_features import delta\n",
    "from python_speech_features import logfbank\n",
    "import scipy.io.wavfile as wav\n",
    "import librosa\n",
    "import sys\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10\n",
      "10.0% complete\n",
      "20.0% complete\n",
      "30.0% complete\n",
      "40.0% complete\n",
      "50.0% complete\n",
      "60.0% complete\n",
      "70.0% complete\n",
      "80.0% complete\n",
      "90.0% complete\n",
      "100.0% complete\n",
      "The one hot signature for this word is: [1 0 0 0 0 0 0 0 0 0]\n",
      "The number of samples for this word is: 1000\n",
      "2/10\n",
      "10.0% complete\n",
      "20.0% complete\n",
      "30.0% complete\n",
      "40.0% complete\n",
      "50.0% complete\n",
      "60.0% complete\n",
      "70.0% complete\n",
      "80.0% complete\n",
      "90.0% complete\n",
      "100.0% complete\n",
      "The one hot signature for this word is: [0 1 0 0 0 0 0 0 0 0]\n",
      "The number of samples for this word is: 1000\n",
      "3/10\n",
      "10.0% complete\n",
      "20.0% complete\n",
      "30.0% complete\n",
      "40.0% complete\n",
      "50.0% complete\n",
      "60.0% complete\n",
      "70.0% complete\n",
      "80.0% complete\n",
      "90.0% complete\n",
      "100.0% complete\n",
      "The one hot signature for this word is: [0 0 1 0 0 0 0 0 0 0]\n",
      "The number of samples for this word is: 1000\n",
      "4/10\n",
      "10.0% complete\n",
      "20.0% complete\n",
      "30.0% complete\n",
      "40.0% complete\n",
      "50.0% complete\n",
      "60.0% complete\n",
      "70.0% complete\n",
      "80.0% complete\n",
      "90.0% complete\n",
      "100.0% complete\n",
      "The one hot signature for this word is: [0 0 0 1 0 0 0 0 0 0]\n",
      "The number of samples for this word is: 1000\n",
      "5/10\n",
      "10.0% complete\n",
      "20.0% complete\n",
      "30.0% complete\n",
      "40.0% complete\n",
      "50.0% complete\n",
      "60.0% complete\n",
      "70.0% complete\n",
      "80.0% complete\n",
      "90.0% complete\n",
      "100.0% complete\n",
      "The one hot signature for this word is: [0 0 0 0 1 0 0 0 0 0]\n",
      "The number of samples for this word is: 1000\n",
      "6/10\n",
      "10.0% complete\n",
      "20.0% complete\n",
      "30.0% complete\n",
      "40.0% complete\n",
      "50.0% complete\n",
      "60.0% complete\n",
      "70.0% complete\n",
      "80.0% complete\n",
      "90.0% complete\n",
      "100.0% complete\n",
      "The one hot signature for this word is: [0 0 0 0 0 1 0 0 0 0]\n",
      "The number of samples for this word is: 1000\n",
      "7/10\n",
      "10.0% complete\n",
      "20.0% complete\n",
      "30.0% complete\n",
      "40.0% complete\n",
      "50.0% complete\n",
      "60.0% complete\n",
      "70.0% complete\n",
      "80.0% complete\n",
      "90.0% complete\n",
      "100.0% complete\n",
      "The one hot signature for this word is: [0 0 0 0 0 0 1 0 0 0]\n",
      "The number of samples for this word is: 1000\n",
      "8/10\n",
      "10.0% complete\n",
      "20.0% complete\n",
      "30.0% complete\n",
      "40.0% complete\n",
      "50.0% complete\n",
      "60.0% complete\n",
      "70.0% complete\n",
      "80.0% complete\n",
      "90.0% complete\n",
      "100.0% complete\n",
      "The one hot signature for this word is: [0 0 0 0 0 0 0 1 0 0]\n",
      "The number of samples for this word is: 1000\n",
      "9/10\n",
      "10.0% complete\n",
      "20.0% complete\n",
      "30.0% complete\n",
      "40.0% complete\n",
      "50.0% complete\n",
      "60.0% complete\n",
      "70.0% complete\n",
      "80.0% complete\n",
      "90.0% complete\n",
      "100.0% complete\n",
      "The one hot signature for this word is: [0 0 0 0 0 0 0 0 1 0]\n",
      "The number of samples for this word is: 1000\n",
      "10/10\n",
      "10.0% complete\n",
      "20.0% complete\n",
      "30.0% complete\n",
      "40.0% complete\n",
      "50.0% complete\n",
      "60.0% complete\n",
      "70.0% complete\n",
      "80.0% complete\n",
      "90.0% complete\n",
      "100.0% complete\n",
      "The one hot signature for this word is: [0 0 0 0 0 0 0 0 0 1]\n",
      "The number of samples for this word is: 1000\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "NUM_LABELS = 10\n",
    "\n",
    "def genOneHot(indexToBeOne):\n",
    "    global NUM_LABELS\n",
    "    oneHot = []\n",
    "    for i in range(NUM_LABELS):\n",
    "        if i == indexToBeOne:\n",
    "            oneHot.append(1)\n",
    "        else:\n",
    "            oneHot.append(0)\n",
    "    return np.array(oneHot)\n",
    "\n",
    "def extractFeatures(raw, sr):\n",
    "    stft = np.abs(librosa.stft(raw))\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=raw, sr=sr, n_mfcc=40).T,axis=0)\n",
    "    mel = np.mean(librosa.feature.melspectrogram(raw, sr=sr).T,axis=0)\n",
    "#     chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sr).T,axis=0)\n",
    "#     contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sr).T,axis=0)\n",
    "#     tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(raw),sr=sr).T,axis=0)\n",
    "    \n",
    "    return mel\n",
    "    \n",
    "def loadSoundFiles(filePath, index):\n",
    "    IDEAL_SIZE = 22050\n",
    "    NUM_SAMPLES = 1000\n",
    "    oneHots = []\n",
    "    soundFiles = os.listdir(filePath)\n",
    "    features = np.empty((0, 144))\n",
    "    \n",
    "    counter = 0\n",
    "    for soundFile in soundFiles:\n",
    "        raw, sr = librosa.load(filePath + soundFile)\n",
    "        if len(raw) == IDEAL_SIZE:\n",
    "            mel = extractFeatures(raw, sr)\n",
    "            mel = np.lib.pad(mel, (8,8), 'constant', constant_values=(0, 0))\n",
    "            internalFeatures = np.hstack([mel])\n",
    "            features = np.vstack([features, internalFeatures])\n",
    "            counter += 1\n",
    "            \n",
    "            oneHot = genOneHot(index)\n",
    "            if len(oneHots) == 0:\n",
    "                oneHots = oneHot\n",
    "            else:\n",
    "                oneHots = np.vstack((oneHots, oneHot))\n",
    "        \n",
    "            if (counter % (NUM_SAMPLES / 10)) == 0:\n",
    "                print(str((counter/NUM_SAMPLES) * 100) + \"% complete\")\n",
    "        \n",
    "            if counter == NUM_SAMPLES:\n",
    "                break\n",
    "            \n",
    "    print(\"The one hot signature for this word is: \" + str(oneHots[0]))\n",
    "    print(\"The number of samples for this word is: \" + str(len(features)))\n",
    "        \n",
    "    return np.array(features), oneHots\n",
    "    \n",
    "data = []\n",
    "oneHots = []\n",
    "rootDir = \"./train/audio/\"\n",
    "acceptedWords = [\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\"]\n",
    "count = 1\n",
    "for acceptedWord in acceptedWords:\n",
    "    print(str(count) + \"/\" + str(NUM_LABELS))\n",
    "    features, oneHot = loadSoundFiles(rootDir + acceptedWord + \"/\", count-1)\n",
    "    data.append(features)\n",
    "    oneHots.append(oneHot)\n",
    "    count += 1\n",
    "    \n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished formatting the data to remove bias and to be ready for training\n"
     ]
    }
   ],
   "source": [
    "dataTraining = []\n",
    "trainingLabels = []\n",
    "dataTest = []\n",
    "testLabels = []\n",
    "\n",
    "minSamples = 1000000\n",
    "for training in data:\n",
    "    if minSamples > len(training):\n",
    "        minSamples = len(training)\n",
    "\n",
    "numTrainingSamples = int(minSamples * 0.8)\n",
    "numTestSamples = int(minSamples * 0.2)\n",
    "\n",
    "for datasubset in data:\n",
    "    if len(dataTraining) == 0:\n",
    "        dataTraining = datasubset[:numTrainingSamples]\n",
    "        dataTest = datasubset[numTrainingSamples:numTrainingSamples+numTestSamples]\n",
    "    else:\n",
    "        dataTraining = np.vstack((dataTraining, datasubset[:numTrainingSamples]))\n",
    "        dataTest = np.vstack((dataTest, datasubset[numTrainingSamples:numTrainingSamples+numTestSamples]))\n",
    "               \n",
    "for oneHotInfo in oneHots:\n",
    "    if len(trainingLabels) == 0:\n",
    "        trainingLabels = oneHotInfo[:numTrainingSamples]\n",
    "        testLabels = oneHotInfo[numTrainingSamples:numTrainingSamples+numTestSamples]\n",
    "    else:\n",
    "        trainingLabels = np.vstack((trainingLabels, oneHotInfo[:numTrainingSamples]))\n",
    "        testLabels = np.vstack((testLabels, oneHotInfo[numTrainingSamples:numTrainingSamples+numTestSamples]))\n",
    "\n",
    "print(\"Finished formatting the data to remove bias and to be ready for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized the order of the data and labels keeping the relationship 1 to 1\n"
     ]
    }
   ],
   "source": [
    "dataTraining, trainingLabels = shuffle(dataTraining, trainingLabels)\n",
    "dataTest, testLabels = shuffle(dataTest, testLabels)\n",
    "print(\"Randomized the order of the data and labels keeping the relationship 1 to 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBatch(data, labels, batchSize):\n",
    "    randomIndexes = np.random.choice(len(data), batchSize)\n",
    "    return data[randomIndexes], labels[randomIndexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1% complete: Training Error = 0.962552. Cross Validation Error = 3.22797\n",
      "2% complete: Training Error = 0.778844. Cross Validation Error = 3.00522\n",
      "3% complete: Training Error = 0.942308. Cross Validation Error = 3.79976\n",
      "4% complete: Training Error = 0.806947. Cross Validation Error = 3.25792\n",
      "5% complete: Training Error = 0.752114. Cross Validation Error = 3.06435\n",
      "6% complete: Training Error = 0.715763. Cross Validation Error = 2.80659\n",
      "7% complete: Training Error = 0.702454. Cross Validation Error = 3.7587\n",
      "8% complete: Training Error = 0.649847. Cross Validation Error = 3.98267\n",
      "9% complete: Training Error = 0.58479. Cross Validation Error = 4.60736\n",
      "10% complete: Training Error = 0.56113. Cross Validation Error = 3.35113\n",
      "11% complete: Training Error = 0.517044. Cross Validation Error = 3.7597\n",
      "12% complete: Training Error = 0.594371. Cross Validation Error = 3.07002\n",
      "13% complete: Training Error = 0.368475. Cross Validation Error = 3.54908\n",
      "14% complete: Training Error = 0.499825. Cross Validation Error = 2.67216\n",
      "15% complete: Training Error = 0.544341. Cross Validation Error = 3.85559\n",
      "16% complete: Training Error = 0.47031. Cross Validation Error = 3.27652\n",
      "17% complete: Training Error = 0.544039. Cross Validation Error = 4.86481\n",
      "18% complete: Training Error = 0.527376. Cross Validation Error = 4.16745\n",
      "19% complete: Training Error = 0.452667. Cross Validation Error = 4.44615\n",
      "20% complete: Training Error = 0.420138. Cross Validation Error = 3.48128\n",
      "21% complete: Training Error = 0.483401. Cross Validation Error = 4.66818\n",
      "22% complete: Training Error = 0.492267. Cross Validation Error = 3.10517\n",
      "23% complete: Training Error = 0.461802. Cross Validation Error = 5.25069\n",
      "24% complete: Training Error = 0.408882. Cross Validation Error = 3.54336\n",
      "25% complete: Training Error = 0.456012. Cross Validation Error = 9.99574\n",
      "26% complete: Training Error = 0.380854. Cross Validation Error = 2.97438\n",
      "27% complete: Training Error = 0.480406. Cross Validation Error = 5.38881\n",
      "28% complete: Training Error = 0.450959. Cross Validation Error = 3.89766\n",
      "29% complete: Training Error = 0.434086. Cross Validation Error = 3.97982\n",
      "30% complete: Training Error = 0.395967. Cross Validation Error = 2.51532\n",
      "31% complete: Training Error = 0.390124. Cross Validation Error = 4.42009\n",
      "32% complete: Training Error = 0.475914. Cross Validation Error = 3.65225\n",
      "33% complete: Training Error = 0.40752. Cross Validation Error = 2.29193\n",
      "34% complete: Training Error = 0.46098. Cross Validation Error = 2.84031\n",
      "35% complete: Training Error = 0.397038. Cross Validation Error = 4.33083\n",
      "36% complete: Training Error = 0.366467. Cross Validation Error = 3.76232\n",
      "37% complete: Training Error = 0.429615. Cross Validation Error = 2.19678\n",
      "38% complete: Training Error = 0.440087. Cross Validation Error = 3.09427\n",
      "39% complete: Training Error = 0.357295. Cross Validation Error = 2.09923\n",
      "40% complete: Training Error = 0.404674. Cross Validation Error = 3.45584\n",
      "41% complete: Training Error = 0.48972. Cross Validation Error = 4.45529\n",
      "42% complete: Training Error = 0.376671. Cross Validation Error = 3.02387\n",
      "43% complete: Training Error = 0.316965. Cross Validation Error = 2.85775\n",
      "44% complete: Training Error = 0.425027. Cross Validation Error = 3.63607\n",
      "45% complete: Training Error = 0.367587. Cross Validation Error = 5.39278\n",
      "46% complete: Training Error = 0.392049. Cross Validation Error = 3.54838\n",
      "47% complete: Training Error = 0.318963. Cross Validation Error = 2.80292\n",
      "48% complete: Training Error = 0.402436. Cross Validation Error = 2.88031\n",
      "49% complete: Training Error = 0.328025. Cross Validation Error = 3.75985\n",
      "50% complete: Training Error = 0.308473. Cross Validation Error = 4.84384\n",
      "51% complete: Training Error = 0.333545. Cross Validation Error = 2.63302\n",
      "52% complete: Training Error = 0.265225. Cross Validation Error = 4.70067\n",
      "53% complete: Training Error = 0.306601. Cross Validation Error = 2.52307\n",
      "54% complete: Training Error = 0.277376. Cross Validation Error = 4.39688\n",
      "55% complete: Training Error = 0.316053. Cross Validation Error = 2.45249\n",
      "56% complete: Training Error = 0.334761. Cross Validation Error = 4.87291\n",
      "57% complete: Training Error = 0.324236. Cross Validation Error = 3.86413\n",
      "58% complete: Training Error = 0.314982. Cross Validation Error = 7.79017\n",
      "59% complete: Training Error = 0.356768. Cross Validation Error = 3.55361\n",
      "60% complete: Training Error = 0.266998. Cross Validation Error = 5.72032\n",
      "61% complete: Training Error = 0.290376. Cross Validation Error = 2.09984\n",
      "62% complete: Training Error = 0.332068. Cross Validation Error = 4.48657\n",
      "63% complete: Training Error = 0.321517. Cross Validation Error = 4.39996\n",
      "64% complete: Training Error = 0.31856. Cross Validation Error = 3.40317\n",
      "65% complete: Training Error = 0.328454. Cross Validation Error = 2.53344\n",
      "66% complete: Training Error = 0.348173. Cross Validation Error = 3.12829\n",
      "67% complete: Training Error = 0.364342. Cross Validation Error = 3.40792\n",
      "68% complete: Training Error = 0.27347. Cross Validation Error = 2.25429\n",
      "69% complete: Training Error = 0.391563. Cross Validation Error = 3.37517\n",
      "70% complete: Training Error = 0.317621. Cross Validation Error = 3.63629\n",
      "71% complete: Training Error = 0.299489. Cross Validation Error = 3.89144\n",
      "72% complete: Training Error = 0.313086. Cross Validation Error = 3.50656\n",
      "73% complete: Training Error = 0.30564. Cross Validation Error = 3.8886\n",
      "74% complete: Training Error = 0.257026. Cross Validation Error = 2.57889\n",
      "75% complete: Training Error = 0.206461. Cross Validation Error = 3.14317\n",
      "76% complete: Training Error = 0.356611. Cross Validation Error = 2.18603\n",
      "77% complete: Training Error = 0.34956. Cross Validation Error = 2.79777\n",
      "78% complete: Training Error = 0.30768. Cross Validation Error = 2.48311\n",
      "79% complete: Training Error = 0.200366. Cross Validation Error = 3.69953\n",
      "80% complete: Training Error = 0.264091. Cross Validation Error = 4.54526\n",
      "81% complete: Training Error = 0.357208. Cross Validation Error = 3.39595\n",
      "82% complete: Training Error = 0.336516. Cross Validation Error = 3.51097\n",
      "83% complete: Training Error = 0.221464. Cross Validation Error = 2.14088\n",
      "84% complete: Training Error = 0.309524. Cross Validation Error = 5.43499\n",
      "85% complete: Training Error = 0.30388. Cross Validation Error = 3.32294\n",
      "86% complete: Training Error = 0.310804. Cross Validation Error = 2.23522\n",
      "87% complete: Training Error = 0.27845. Cross Validation Error = 2.71038\n",
      "88% complete: Training Error = 0.306849. Cross Validation Error = 2.70721\n",
      "89% complete: Training Error = 0.271993. Cross Validation Error = 3.48839\n",
      "90% complete: Training Error = 0.229217. Cross Validation Error = 2.03998\n",
      "91% complete: Training Error = 0.216171. Cross Validation Error = 2.81747\n",
      "92% complete: Training Error = 0.450258. Cross Validation Error = 7.81466\n",
      "93% complete: Training Error = 0.306015. Cross Validation Error = 3.05393\n",
      "94% complete: Training Error = 0.326581. Cross Validation Error = 2.94761\n",
      "95% complete: Training Error = 0.227898. Cross Validation Error = 2.53749\n",
      "96% complete: Training Error = 0.3312. Cross Validation Error = 3.57125\n",
      "97% complete: Training Error = 0.298995. Cross Validation Error = 3.70482\n",
      "98% complete: Training Error = 0.239836. Cross Validation Error = 2.81838\n",
      "99% complete: Training Error = 0.25105. Cross Validation Error = 2.18441\n",
      "100% complete: Training Error = 0.311728. Cross Validation Error = 3.47236\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "NUM_SAMPLES = 144\n",
    "NUM_CLASSIFICATIONS = NUM_LABELS\n",
    "FIRST_LAYER_OUTPUT = 100\n",
    "RATE = 0.01\n",
    "BATCH_SIZE = 200\n",
    "EPOCHS = 5000\n",
    "ITERATIONS_PER_EPOCH = 100\n",
    "\n",
    "# Shape = (N x 128)\n",
    "x = tf.placeholder(tf.float32, (None, NUM_SAMPLES))\n",
    "# Shape = (N x 2)\n",
    "ref = tf.placeholder(tf.float32, (None, NUM_CLASSIFICATIONS))\n",
    "# Calculate the logits\n",
    "logits1 = tf.layers.dense(inputs=x, units=FIRST_LAYER_OUTPUT, activation=tf.nn.relu, name=\"L1\")\n",
    "logits2 = tf.layers.dense(inputs=logits1, units=NUM_CLASSIFICATIONS, activation=None, name=\"L2\")\n",
    "\n",
    "# The mean cross entropy as the cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits2, labels=ref))\n",
    "\n",
    "# Initialize the tensorflow session\n",
    "optimizer = tf.train.GradientDescentOptimizer(RATE).minimize(cost)\n",
    "s = tf.Session()\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "# Do the training\n",
    "count = 0\n",
    "for _ in range(EPOCHS):\n",
    "    inputData, correctAns = getBatch(dataTraining, trainingLabels, BATCH_SIZE)\n",
    "    crossData, crossAns = getBatch(dataTraining, trainingLabels, BATCH_SIZE)\n",
    "    for _ in range(ITERATIONS_PER_EPOCH):\n",
    "        err, _ = s.run((cost, optimizer), feed_dict={x: inputData, ref: correctAns})\n",
    "        count += 1\n",
    "        if (count % ((EPOCHS * ITERATIONS_PER_EPOCH) / 100)) == 0:\n",
    "            crossErr = s.run((cost), feed_dict={x: crossData, ref: crossAns})\n",
    "            print(str(int(((count / (EPOCHS * ITERATIONS_PER_EPOCH)) * 100) + 0.5)) + \"% complete: Training Error = \" + str(err) + \". Cross Validation Error = \" + str(crossErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logits2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-294-c60ccafb62a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Take the softmax to convert the logit value to a percentage guess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprobability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Take the highest probability value as the neural network's guess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Check how accurate the training is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logits2' is not defined"
     ]
    }
   ],
   "source": [
    "# Take the softmax to convert the logit value to a percentage guess\n",
    "probability = tf.nn.softmax(logits2)\n",
    "# Take the highest probability value as the neural network's guess\n",
    "prediction = tf.argmax((probability), axis=1)\n",
    "# Check how accurate the training is\n",
    "guesses = s.run((prediction), feed_dict={x: dataTraining})\n",
    "\n",
    "count = 0\n",
    "corr = 0\n",
    "categoriesRight = np.zeros(10)\n",
    "for trainingLabel in trainingLabels:\n",
    "    if (np.argmax(trainingLabel)) == guesses[count]:\n",
    "        corr += 1\n",
    "        categoriesRight[np.argmax(trainingLabel)] += 1\n",
    "    count += 1\n",
    "\n",
    "print(\"The mlp neural network is \" + str((corr / len(guesses)) * 100) + \"% accurate on the training data\")\n",
    "print(acceptedWords)\n",
    "count = 0\n",
    "print((categoriesRight / numTrainingSamples) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The neural network is 40.25% accurate on the test data\n",
      "['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
      "[ 60.   34.   41.5  34.   34.   32.   37.   51.5  37.   41.5]\n"
     ]
    }
   ],
   "source": [
    "# Take the softmax to convert the logit value to a percentage guess\n",
    "probability = tf.nn.softmax(logits2)\n",
    "# Take the highest probability value as the neural network's guess\n",
    "prediction = tf.argmax((probability), axis=1)\n",
    "# Check how accurate the test is\n",
    "guesses = s.run((prediction), feed_dict={x: dataTest})\n",
    "\n",
    "count = 0\n",
    "corr = 0\n",
    "categoriesRight = np.zeros(10)\n",
    "for testLabel in testLabels:\n",
    "    if (np.argmax(testLabel)) == guesses[count]:\n",
    "        corr += 1\n",
    "        categoriesRight[np.argmax(testLabel)] += 1\n",
    "    count += 1\n",
    "    \n",
    "print(\"The mlp neural network is \" + str((corr / len(guesses)) * 100) + \"% accurate on the test data\")\n",
    "print(acceptedWords)\n",
    "print((categoriesRight / numTestSamples) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnnTraining = dataTraining\n",
    "cnnTrainingLabels = trainingLabels\n",
    "cnnTest = dataTest\n",
    "cnnTestLabels = testLabels\n",
    "\n",
    "def fixShape(data):\n",
    "    return data.reshape(-1, 1, 144, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1% complete: Training Error = 1.16044. Cross Validation Error = 2.36456\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-300-f7ab9194f61d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mcrossData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfixShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrossData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mITERATIONS_PER_EPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcorrectAns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDROP_OUT_RATE\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mITERATIONS_PER_EPOCH\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "NUM_SAMPLES = 144\n",
    "DENSE_LAYER = 256\n",
    "NUM_CLASSIFICATIONS = NUM_LABELS\n",
    "RATE = 0.01\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 5000\n",
    "ITERATIONS_PER_EPOCH = 100\n",
    "DROP_OUT_RATE = 0.5\n",
    "\n",
    "class N():\n",
    "    pass\n",
    "\n",
    "model = N()\n",
    "\n",
    "model.x = tf.placeholder(tf.float32, (None, 1, None, 1))\n",
    "model.ref = tf.placeholder(tf.float32, (None, NUM_CLASSIFICATIONS))\n",
    "model.drop = tf.placeholder(tf.float32)\n",
    "model.L1 = tf.layers.conv2d(model.x, filters=32, kernel_size=(1,5), padding=\"SAME\", activation=tf.nn.relu)\n",
    "model.L2 = tf.layers.max_pooling2d(inputs=model.L1, pool_size=(1,2), strides=(1, 2))\n",
    "model.L3 = tf.reshape(model.L2, (-1, 32*72))\n",
    "model.L4 = tf.layers.dense(inputs=model.L3, units=DENSE_LAYER)\n",
    "model.L5 = tf.nn.dropout(model.L4, model.drop)\n",
    "model.L6 = tf.layers.dense(inputs=model.L5, units=NUM_CLASSIFICATIONS)\n",
    "model.err = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model.L6, labels=model.ref))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(RATE).minimize(model.err)\n",
    "s = tf.Session()\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "count = 0\n",
    "for _ in range(EPOCHS):\n",
    "    inputData, correctAns = getBatch(cnnTraining, cnnTrainingLabels, BATCH_SIZE)\n",
    "    inputData = fixShape(inputData)\n",
    "    crossData, crossAns = getBatch(cnnTraining, cnnTrainingLabels, BATCH_SIZE)\n",
    "    crossData = fixShape(crossData)\n",
    "    for _ in range(ITERATIONS_PER_EPOCH):\n",
    "        err, _ = s.run((model.err, optimizer), feed_dict={model.x: inputData, model.ref: correctAns, model.drop: DROP_OUT_RATE})\n",
    "        count += 1\n",
    "        if (count % ((EPOCHS * ITERATIONS_PER_EPOCH) / 100)) == 0:\n",
    "            crossErr = s.run((model.err), feed_dict={model.x: crossData, model.ref: crossAns, model.drop: DROP_OUT_RATE})\n",
    "            print(str(int(((count / (EPOCHS * ITERATIONS_PER_EPOCH)) * 100) + 0.5)) + \"% complete: Training Error = \" + str(err) + \". Cross Validation Error = \" + str(crossErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cnn neural network is 10.0% accurate on the training data\n",
      "['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
      "[   0.    0.    0.    0.    0.    0.    0.    0.    0.  100.]\n"
     ]
    }
   ],
   "source": [
    "# Take the softmax to convert the logit value to a percentage guess\n",
    "probability = tf.nn.softmax(model.L6)\n",
    "# Take the highest probability value as the neural network's guess\n",
    "prediction = tf.argmax((probability), axis=1)\n",
    "# Check how accurate the training is\n",
    "guesses = s.run((prediction), feed_dict={model.x: fixShape(cnnTraining)})\n",
    "\n",
    "count = 0\n",
    "corr = 0\n",
    "categoriesRight = np.zeros(10)\n",
    "for cnnTrainingLabel in cnnTrainingLabels:\n",
    "    if (np.argmax(cnnTrainingLabel)) == guesses[count]:\n",
    "        corr += 1\n",
    "        categoriesRight[np.argmax(cnnTrainingLabel)] += 1\n",
    "    count += 1\n",
    "\n",
    "print(\"The cnn neural network is \" + str((corr / len(guesses)) * 100) + \"% accurate on the training data\")\n",
    "print(acceptedWords)\n",
    "count = 0\n",
    "print((categoriesRight / numTrainingSamples) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cnn neural network is 10.0% accurate on the test data\n",
      "['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
      "[   0.    0.    0.    0.    0.    0.    0.    0.    0.  100.]\n"
     ]
    }
   ],
   "source": [
    "# Take the softmax to convert the logit value to a percentage guess\n",
    "probability = tf.nn.softmax(model.L6)\n",
    "# Take the highest probability value as the neural network's guess\n",
    "prediction = tf.argmax((probability), axis=1)\n",
    "# Check how accurate the test is\n",
    "guesses = s.run((prediction), feed_dict={model.x: fixShape(cnnTest)})\n",
    "\n",
    "count = 0\n",
    "corr = 0\n",
    "categoriesRight = np.zeros(10)\n",
    "for cnnTestLabel in cnnTestLabels:\n",
    "    if (np.argmax(cnnTestLabel)) == guesses[count]:\n",
    "        corr += 1\n",
    "        categoriesRight[np.argmax(cnnTestLabel)] += 1\n",
    "    count += 1\n",
    "    \n",
    "print(\"The cnn neural network is \" + str((corr / len(guesses)) * 100) + \"% accurate on the test data\")\n",
    "print(acceptedWords)\n",
    "print((categoriesRight / numTestSamples) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
