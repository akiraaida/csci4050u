{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question F\n",
    "Increase your network size with additional neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question F.1\n",
    "[10] Repeat the training and evaluate the test error and compare with (E.5).  Comment on your observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "%matplotlib inline\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "numPixels = 784\n",
    "layerOutput = 100\n",
    "numClassifications = 10\n",
    "\n",
    "# Initial input\n",
    "x = tf.placeholder(tf.float32, [None, numPixels])\n",
    "# Reference\n",
    "ref = tf.placeholder(tf.float32, [None, numClassifications])\n",
    "\n",
    "logits1 = tf.layers.dense(inputs=x, units=layerOutput, activation=None, name=\"L1\")\n",
    "logits2 = tf.layers.dense(inputs=logits1, units=layerOutput, activation=None, name=\"L2\")\n",
    "logits3 = tf.layers.dense(inputs=logits2, units=layerOutput, activation=None, name=\"L3\")\n",
    "logits4 = tf.layers.dense(inputs=logits3, units=layerOutput, activation=None, name=\"L4\")\n",
    "logits5 = tf.layers.dense(inputs=logits4, units=layerOutput, activation=None, name=\"L5\")\n",
    "logits6 = tf.layers.dense(inputs=logits5, units=layerOutput, activation=None, name=\"L6\")\n",
    "logits7 = tf.layers.dense(inputs=logits6, units=layerOutput, activation=None, name=\"L7\")\n",
    "logits8 = tf.layers.dense(inputs=logits7, units=layerOutput, activation=None, name=\"L8\")\n",
    "logits9 = tf.layers.dense(inputs=logits8, units=layerOutput, activation=None, name=\"L9\")\n",
    "logits10 = tf.layers.dense(inputs=logits9, units=numClassifications, activation=None, name=\"L10\")\n",
    "\n",
    "# Calculate the cross entropy, doing the softmax function internally\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits10, labels=ref)\n",
    "\n",
    "# Take the average of the cross entropy values\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# (N). Take the highest percentile value in the prediction as the answer\n",
    "prediction = tf.argmax(tf.nn.softmax(logits10), axis=1)\n",
    "\n",
    "labels = []\n",
    "for label in mnist.test.labels:\n",
    "    labels.append(np.argmax(label))\n",
    "labels = np.array(labels)\n",
    "\n",
    "rate = 0.01\n",
    "epochs = 100\n",
    "iterationsPerEpoch = 10\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(rate).minimize(cost)\n",
    "s = tf.Session()\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "errList = []\n",
    "validationErrList = []\n",
    "epochList = []\n",
    "inputData, correctAns = mnist.train.next_batch(100)\n",
    "validationInput, validationCorrect = mnist.train.next_batch(100)\n",
    "\n",
    "count = 0\n",
    "for _ in range(epochs):\n",
    "    inputData, correctAns = mnist.train.next_batch(100)\n",
    "    validationInput, validationCorrect = mnist.train.next_batch(100)\n",
    "    for _ in range(iterationsPerEpoch):\n",
    "        # Do the training\n",
    "        err, _ = s.run((cost, optimizer), feed_dict={x: inputData, ref: correctAns})\n",
    "\n",
    "        # Do the validation\n",
    "        validationErr = s.run((cost), feed_dict={x: validationInput, ref: validationCorrect})\n",
    "        validationErrList.append(validationErr)\n",
    "                \n",
    "        errList.append(err)\n",
    "        epochList.append(count)\n",
    "        count += 1\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test error for the final iteration of the training is: 13.23 percent.\n"
     ]
    }
   ],
   "source": [
    "# Check the test error\n",
    "predic = s.run((prediction), feed_dict={x: mnist.test.images})\n",
    "answers = np.equal(labels, predic)\n",
    "corrGuesses = np.sum(answers)\n",
    "testErr = corrGuesses / len(answers)   \n",
    "print(\"The test error for the final iteration of the training is: \" + str((1 - testErr) * 100) + \" percent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test error for the final iteration of the training is actually around the same as E.5. This would make sense to a degree since significant overfitting can be seen in the cross validation graph of E.5. Where the number of neurons does not affect the overfitting error due to the number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
